"""
å¤šå‚å•†LLMæœåŠ¡ - åŠŸèƒ½æµ‹è¯•æ¨¡å—
æ”¯æŒOpenAIã€Anthropicã€é€šä¹‰åƒé—®ã€æ–‡å¿ƒä¸€è¨€
"""

from typing import Any, Optional

from langchain_anthropic import ChatAnthropic
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_openai import ChatOpenAI
from sqlalchemy.ext.asyncio import AsyncSession
from sqlmodel import select

from app.models.ai_config import AIProviderConfig
from app.services.ai_config_service import AIConfigService


class MultiVendorLLMService:
    """å¤šå‚å•†LLMæœåŠ¡"""

    def __init__(self, config: AIProviderConfig):
        """
        åˆå§‹åŒ–LLMæœåŠ¡

        Args:
            config: AIå‚å•†é…ç½®å¯¹è±¡
        """
        self.config = config
        self._provider_type = config.provider_type
        self._decrypted_api_key = AIConfigService.decrypt_config_key(config)

    def get_llm(self) -> Any:
        """
        è·å–LLMå®ä¾‹

        Returns:
            LLMå®ä¾‹ï¼ˆlangchainèŠå¤©æ¨¡å‹ï¼‰
        """
        provider_type = self._provider_type

        # æ„å»ºé€šç”¨å‚æ•°
        kwargs = {
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        }

        # æ ¹æ®å‚å•†ç±»å‹åˆ›å»ºå¯¹åº”çš„LLMå®ä¾‹
        if provider_type == "openai":
            return ChatOpenAI(
                model=self.config.model_name,
                openai_api_key=self._decrypted_api_key,
                base_url=self.config.api_endpoint,
                **kwargs,
            )

        elif provider_type == "anthropic":
            return ChatAnthropic(
                model=self.config.model_name,
                anthropic_api_key=self._decrypted_api_key,
                base_url=self.config.api_endpoint,
                **kwargs,
            )

        elif provider_type == "qwen":
            # é˜¿é‡Œäº‘é€šä¹‰åƒé—®
            from langchain_community.chat_models.tongyi import ChatTongyi

            return ChatTongyi(
                dashscope_api_key=self._decrypted_api_key,
                model_name=self.config.model_name,
                **kwargs,
            )

        elif provider_type == "qianfan":
            # ç™¾åº¦æ–‡å¿ƒä¸€è¨€
            return QianfanChatEndpoint(
                qianfan_api_key=self._decrypted_api_key, model=self.config.model_name, **kwargs
            )

        elif provider_type == "glm":
            # æ™ºè°±AI (GLM) - ä½¿ç”¨ä¸“ç”¨å®ç°
            from langchain_community.chat_models import ChatZhipuAI

            return ChatZhipuAI(
                api_key=self._decrypted_api_key, model=self.config.model_name, **kwargs
            )

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIå‚å•†: {provider_type}")

    @staticmethod
    async def get_default_llm_service(
        session: AsyncSession, user_id: int
    ) -> Optional["MultiVendorLLMService"]:
        """
        è·å–ç”¨æˆ·çš„é»˜è®¤LLMæœåŠ¡å®ä¾‹

        Args:
            session: æ•°æ®åº“ä¼šè¯
            user_id: ç”¨æˆ·ID

        Returns:
            MultiVendorLLMServiceå®ä¾‹æˆ–Noneï¼ˆå¦‚æœæ²¡æœ‰é»˜è®¤é…ç½®ï¼‰
        """
        import sys

        print(f"ğŸ” [get_default_llm] å¼€å§‹è·å–ç”¨æˆ· {user_id} çš„é»˜è®¤LLM", file=sys.stderr, flush=True)

        # è·å–é»˜è®¤é…ç½®
        result = await session.execute(
            select(AIProviderConfig)
            .where(AIProviderConfig.user_id == user_id)
            .where(AIProviderConfig.is_default)
            .where(AIProviderConfig.is_enabled)
        )
        config = result.scalar_one_or_none()

        print(f"ğŸ” [get_default_llm] æŸ¥è¯¢ç»“æœ: {config}", file=sys.stderr, flush=True)

        if not config:
            print("âŒ [get_default_llm] æœªæ‰¾åˆ°é…ç½®", file=sys.stderr, flush=True)
            return None

        # åˆ›å»ºå¹¶è¿”å›LLMæœåŠ¡å®ä¾‹
        try:
            print(
                f"ğŸ—ï¸ [get_default_llm] åˆ›å»º LLM æœåŠ¡ï¼Œprovider_type={config.provider_type}",
                file=sys.stderr,
                flush=True,
            )
            llm_service = MultiVendorLLMService(config)
            print(
                f"âœ… [get_default_llm] LLM æœåŠ¡åˆ›å»ºæˆåŠŸ: {type(llm_service).__name__}",
                file=sys.stderr,
                flush=True,
            )
            return llm_service
        except Exception as e:
            print(f"âŒ [get_default_llm] åˆ›å»º LLM å¤±è´¥: {e}", file=sys.stderr, flush=True)
            import traceback

            traceback.print_exc(file=sys.stderr)
            return None

    @staticmethod
    async def get_llm_by_provider(
        session: AsyncSession, user_id: int, provider_type: str
    ) -> Any | None:
        """
        æ ¹æ®å‚å•†ç±»å‹è·å–LLMå®ä¾‹

        Args:
            session: æ•°æ®åº“ä¼šè¯
            user_id: ç”¨æˆ·ID
            provider_type: å‚å•†ç±»å‹

        Returns:
            LLMå®ä¾‹æˆ–None
        """
        # è·å–æŒ‡å®šå‚å•†çš„é…ç½®
        result = await session.execute(
            select(AIProviderConfig)
            .where(AIProviderConfig.user_id == user_id)
            .where(AIProviderConfig.provider_type == provider_type)
            .where(AIProviderConfig.is_enabled)
            .order_by(AIProviderConfig.is_default.desc())
        )
        config = result.scalar_one_or_none()

        if not config:
            return None

        # åˆ›å»ºå¹¶è¿”å›LLMå®ä¾‹
        llm_service = MultiVendorLLMService(config)
        return llm_service.get_llm()

    async def ainvoke(self, messages: list[dict[str, str]]) -> str:
        """
        å¼‚æ­¥è°ƒç”¨LLM

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]

        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        import sys

        llm = self.get_llm()
        print(
            f"[ainvoke] å¼€å§‹å¼‚æ­¥è°ƒç”¨, provider_type={self._provider_type}",
            file=sys.stderr,
            flush=True,
        )

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

        lc_messages = []
        for msg in messages:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                lc_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                lc_messages.append(AIMessage(content=msg["content"]))

        print(f"[ainvoke] æ¶ˆæ¯è½¬æ¢å®Œæˆ, æ¶ˆæ¯æ•°é‡={len(lc_messages)}", file=sys.stderr, flush=True)

        # å¯¹äº qwen (ChatTongyi)ï¼Œä½¿ç”¨ to_thread åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥è°ƒç”¨
        if self._provider_type == "qwen":
            import asyncio

            print("[ainvoke] ä½¿ç”¨ asyncio.to_thread è°ƒç”¨åŒæ­¥ invoke", file=sys.stderr, flush=True)
            try:
                response = await asyncio.to_thread(llm.invoke, lc_messages)
                print(
                    f"[ainvoke] asyncio.to_thread å®Œæˆ, å“åº”é•¿åº¦={len(response.content)}",
                    file=sys.stderr,
                    flush=True,
                )
                return response.content
            except Exception as e:
                print(f"[ainvoke] asyncio.to_thread å¤±è´¥: {e}", file=sys.stderr, flush=True)
                import traceback

                traceback.print_exc(file=sys.stderr)
                raise
        else:
            # å…¶ä»–å‚å•†ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
            print("[ainvoke] ä½¿ç”¨å¼‚æ­¥ ainvoke", file=sys.stderr, flush=True)
            response = await llm.ainvoke(lc_messages)
            return response.content

    async def astream(self, messages: list[dict[str, str]]):
        """
        å¼‚æ­¥æµå¼è°ƒç”¨LLM

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Yields:
            LLMå“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        llm = self.get_llm()

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        from langchain.schema import AIMessage, HumanMessage, SystemMessage

        lc_messages = []
        for msg in messages:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                lc_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                lc_messages.append(AIMessage(content=msg["content"]))

        # æµå¼è°ƒç”¨LLM
        async for chunk in llm.astream(lc_messages):
            yield chunk.content

    def get_model_info(self) -> dict[str, Any]:
        """
        è·å–å½“å‰æ¨¡å‹ä¿¡æ¯

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "provider_type": self._provider_type,
            "provider_name": self.config.provider_name,
            "model_name": self.config.model_name,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        }


# ä½¿ç”¨ç¤ºä¾‹
"""
from app.services.ai.llm_service import MultiVendorLLMService
from sqlalchemy.ext.asyncio import AsyncSession

async def example_usage():
    # æ–¹å¼1: ä½¿ç”¨é…ç½®å¯¹è±¡åˆ›å»º
    config = ... # ä»æ•°æ®åº“è·å–é…ç½®
    llm_service = MultiVendorLLMService(config)
    llm = llm_service.get_llm()
    response = await llm.ainvoke([{"role": "user", "content": "Hello"}])

    # æ–¹å¼2: ç›´æ¥è·å–ç”¨æˆ·é»˜è®¤LLM
    llm = await MultiVendorLLMService.get_default_llm(session, user_id)
    if llm:
        response = await llm.ainvoke([{"role": "user", "content": "Hello"}])

    # æ–¹å¼3: æ ¹æ®å‚å•†ç±»å‹è·å–
    llm = await MultiVendorLLMService.get_llm_by_provider(session, user_id, "openai")
"""
